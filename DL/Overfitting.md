# Overfitting

클래스가 1개인 경우.

일반적으로 클래스가 하나인 단일 출력 회귀 문제에서는 출력 레이어에 활성화 함수로는 주로 활성화 함수를 사용하지 않거나, 선형(linear) 활성화 함수를 사용한다.

- 단일클래스에서 softmax로 인한 과적합 사례.

활성화 함수를 사용하지 않거나 선형 활성화 함수를 사용하는 이유는 다음과 같습니다:

1. **출력 범위**: 회귀 문제에서는 예측값이 어떤 범위의 실수값일 수 있습니다. 따라서 출력 레이어의 활성화 함수를 적용하면 출력 범위가 제한될 수 있으며, 이는 모델의 유연성을 제한할 수 있습니다. 선형 활성화 함수를 사용하면 출력 값의 범위가 제한되지 않아 모델이 더 유연하게 출력을 조정할 수 있습니다.
2. **Gradient Vanishing/Exploding 문제**: 출력 레이어에 relu와 같은 활성화 함수를 사용하면 역전파(backpropagation) 과정에서 그래디언트 소실(vanishing gradient)이나 그래디언트 폭발(exploding gradient)과 같은 문제가 발생할 수 있습니다. 선형 활성화 함수를 사용하면 이러한 문제를 방지할 수 있습니다.

따라서 클래스가 하나인 회귀 문제에서는 일반적으로 출력 레이어에 활성화 함수를 적용하지 않거나 선형 활성화 함수를 적용하는 것이 더 적합합니다. 이렇게 함으로써 모델이 예측값을 자유롭게 조정할 수 있고, 학습 과정에서 그래디언트가 잘 전파될 수 있습니다.