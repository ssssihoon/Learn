# Regression

## 회귀

- 연속값
- 최적의 회귀 모델
    - 전체 데이터의 잔차(오류 값) 합이 최소가 되는 모델 || 오류 값 합이 최소가 될 수 있는 최적의 회귀 계수를 찾기
    
    → RSS 방식 : 오류 값의 제곱을 구해서 더하는 방식
    

| 독립변수 개수 | 회귀 개수의 결합 |
| --- | --- |
| 1개 : 단일회귀 | 선형 : 선형회귀 |
| 여러 개 : 다중회귀 | 비선형 : 비선형 회귀 |
- L1규제 : 예측 영향력이 작은 피처의 회귀 계수를 0으로 만들어 회귀 예측 시 피처가 선택되지 않게 하는 것
- L2규제 : 상대적으로 큰 회귀 계수 값의 예측 영향도를 감소시키기 위해 회귀 계수값을 더 작게 만드는 규제 모델

---

- Ridge(릿지) : 선형 회귀에 L2 규제를 추가한 회귀 모델
- Lasso(라쏘) : 선형 회귀에 L1 규제를 적용한 방식
- ElasticNet(엘라스틱넷) : L2, L1 규제를 함께 결합한 모델, 주로 피처가 많은 데이터 세트에서 적용
- Logistic Regression(로지스틱 회귀) : 분류에 사용되는 회귀 모델
    
    

## 경사 하강법 (Gradient Descent)

- 고차원 방정식에 대한 문제를 해결, 비용함수 RSS를 최소화하는 방법을 직관적으로 제공
- 점진적으로 반복적인 계산을 통해 W파라미터 값을 업데이트하면서 오류 값이 최소가 되는 W파라미터를 구하는 방식
- 프로세스
    1. w1, w0를 임의로 값을 설정하고 첫 비용 함수의 값을 계산
    2. w1을 업데이트한 후 다시 비용 함수의 값을 계산
    3. 비용 함수가 감소하는 방향성으로 주어진 횟수만큼 2를 반복해 업데이트
